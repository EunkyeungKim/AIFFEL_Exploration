{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54b260b6",
   "metadata": {},
   "source": [
    "### 데이터 불러오기\n",
    "\n",
    "필요한 라이브러리와 모듈을 임포트하고 제공받은 데이터 파일은 경로로 불러와 변수에 저장해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d32f711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob  # glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환ㅠ\n",
    "import os, re # os: 운영체제를 제어 # re: 정규 표현식 지원\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# 파일을 불러와 txt_file_path에 저장한다\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담는다.\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:   # 일기 모드로 파일 열기\n",
    "        raw = f.read().splitlines()   # 라인 단위로 끊어서 리스트로 읽기\n",
    "        raw_corpus.extend(raw)        # 읽은 애들 하나씩 raw_corpus추가하기\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a298829",
   "metadata": {},
   "source": [
    " ### 데이터 정제하기\n",
    " \n",
    "정규표현식을 사용하여 우리에게 필요한 데이터만 정제하였습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3026695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # re는 정규표현식을 지원하는 모듈을 의미함\n",
    "    sentence = sentence.lower().strip() # 소문자 변환, 양쪽 공백 지우기\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽에 공백을 넣고\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "    sentence = sentence.strip() \n",
    "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c76ba7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장이 들어간다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:           # raw_corpus에 저장한 리스트가  sentence 안에 있는동안\n",
    "    if len(sentence) == 0: continue  # 문장이 공백인건 건너뛰세요\n",
    "    if sentence[-1] == \":\": continue # 문장 끝이 \":\"인건 건너뛰세요\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c634c",
   "metadata": {},
   "source": [
    "### 토큰화 하기\n",
    "\n",
    "컴퓨터는 단어를 그대로 못 읽기 때문에 컴퓨터가 이해할 수 있는 언어인 숫자로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb1f37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2639 ...    0    0    0]\n",
      " [   2   36    7 ...   43    3    0]\n",
      " ...\n",
      " [   2    5  107 ...    0    0    0]\n",
      " [   2  261  200 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f93b0314400>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용한다\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, # 단어장의 크기는 12,000 이상 으로 설정\n",
    "        filters=\" \",     # 문장을 정제했으르모 filters 설정 안해줌\n",
    "        oov_token=\"<unk>\") #12000 단어에 포함되지 못한 단어는 \"<unk>\"로 바꿔준다\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)    # corpus를 이용해 tokenizer 내부의 단어장을 완성한다\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)  # tokenizer를 이용해 corpus를 Tensor로 변환한다\n",
    "    \n",
    "    # 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외한다\n",
    "    tensor = [i for i in tensor if len(i) <= 15] \n",
    "\n",
    "    # 입력 데이터의 문장 길이를 일정하게 맞춰준다\n",
    "    # 만약 문장 길이가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰준다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용한다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding=\"post\")\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus) # tokenize() 함수로 데이터를 Tensor로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cbf08aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "11 : it\n",
      "12 : me\n",
      "13 : my\n",
      "14 : in\n",
      "15 : t\n",
      "16 : s\n",
      "17 : that\n",
      "18 : on\n",
      "19 : of\n",
      "20 : .\n"
     ]
    }
   ],
   "source": [
    "# 토큰화가 잘 되었는지 확인해보자\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 20: break  # 여러개 확인해보고 싶어서 20개로 지정해보았따\n",
    "        \n",
    "# 잘 된걸 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b701f45",
   "metadata": {},
   "source": [
    "### 평가 데이터셋 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6daa48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0]\n",
      "[  50    5   91  297   65   57    9  969 6042    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51757e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9601874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train:  (124810, 14)\n",
      "Target Train:  (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "print('Source Train: ', enc_train.shape)\n",
    "print('Target Train: ', dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a8af4",
   "metadata": {},
   "source": [
    "### 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5919993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class enerate_text(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(enerate_text, self).__init__()\n",
    "        \n",
    "        #입력된 텐서에는 단어사전의 인덱스가 들어있는데, 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다.\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "\n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 2048\n",
    "lyricist = enerate_text(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebff9b1",
   "metadata": {},
   "source": [
    "### 모델 학습 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b285a966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 512\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "# tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))  # 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29b8f80f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "609/609 [==============================] - 320s 491ms/step - loss: 3.2927 - val_loss: 2.8937\n",
      "Epoch 2/10\n",
      "609/609 [==============================] - 317s 520ms/step - loss: 2.7776 - val_loss: 2.5809\n",
      "Epoch 3/10\n",
      "609/609 [==============================] - 318s 522ms/step - loss: 2.5155 - val_loss: 2.3118\n",
      "Epoch 4/10\n",
      "609/609 [==============================] - 318s 523ms/step - loss: 2.2733 - val_loss: 2.0627\n",
      "Epoch 5/10\n",
      "609/609 [==============================] - 318s 523ms/step - loss: 2.0449 - val_loss: 1.8325\n",
      "Epoch 6/10\n",
      "609/609 [==============================] - 318s 522ms/step - loss: 1.8309 - val_loss: 1.6232\n",
      "Epoch 7/10\n",
      "609/609 [==============================] - 318s 523ms/step - loss: 1.6333 - val_loss: 1.4374\n",
      "Epoch 8/10\n",
      "609/609 [==============================] - 323s 531ms/step - loss: 1.4572 - val_loss: 1.2802\n",
      "Epoch 9/10\n",
      "609/609 [==============================] - 326s 534ms/step - loss: 1.3081 - val_loss: 1.1554\n",
      "Epoch 10/10\n",
      "609/609 [==============================] - 326s 535ms/step - loss: 1.1887 - val_loss: 1.0664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f92ec4c6430>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련에 사용할 옵티마이저(optimizer)와 손실 함수를 선택합니다:\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "lyricist.compile(loss=loss, optimizer=optimizer)\n",
    "lyricist.fit(dataset, epochs=10, validation_data=(enc_val, dec_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277f25f",
   "metadata": {},
   "source": [
    "### 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "011af2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lyricist, tokenizer, init_sentence=\"<start> \", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    \n",
    "    while True:\n",
    "        # 1. 입력받은 문장의 텐서를 입력합니다\n",
    "        predict = lyricist(test_tensor) \n",
    "        \n",
    "        # 2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        \n",
    "        # 3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        \n",
    "        if enerate_text.numpy()[0] == end_token: break  # 4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    " \n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba05c157",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14/3598535403.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlyricist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_sentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<start> i love\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14/2778247564.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(lyricist, tokenizer, init_sentence, max_len)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtest_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0menerate_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# 4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)\n",
    "print(generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2e3e3",
   "metadata": {},
   "source": [
    "회고\n",
    "\n",
    "점점 어려워진다. 이번에 어려웠던건 단어 길이 제한하는 거랑 val loss 구하는 거였다.\n",
    "이유는,, 초반에 이걸로 엄청 해맸기 때문이다.\n",
    "팀원님들의 도움으로 for문과 if 문으로 잘 해결할 수 있었다! 분명히 어떻게 하는지 아는데\n",
    "막상 적용하면 왜이렇게 오류가 날까?\n",
    "그리고 틀려도 도대체 어디서 뭐가 틀린건지 도저히 모르겠는 것도 많았다.\n",
    "\n",
    "맨 마지막거는 ,,\n",
    "검색을 해보고 고치기도 하고 책도 찾아보고 아,, \n",
    "뭐가 틀렸길래 대체 가사가 안나올까 정말 모르겠다.\n",
    "ㅜ.ㅜ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ea820",
   "metadata": {},
   "source": [
    "참고\n",
    "\n",
    "    \n",
    "https://www.tensorflow.org/tutorials/keras/overfit_and_underfit?hl=ko#%ED%81%B0_%EB%AA%A8%EB%8D%B8_%EB%A7%8C%EB%93%A4%EA%B8%B0\n",
    "    \n",
    "https://github.com/nameunji/aiffel_project/blob/main/lyricist/lyricist.py\n",
    "\n",
    "https://daje0601.tistory.com/196?category=952899\n",
    "\n",
    "https://velog.io/@p2yeong/Over-fitting-vs.-Under-fitting\n",
    "\n",
    "다른데서 본것도 있는데 하다가 지워버려서 찾을수가 없게 되었다 ㅠㅠ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eee0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
